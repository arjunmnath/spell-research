batch_size: 8
epochs: 4
lr: 3e-5 (with linear Scheduling and warmup)
optimiser: adamw
embedding: microsoft/deberta-v3-large
accelerator: 2x T4 GPU
token_len: 256
embedding_pooling: mean max pool + [cls] token
cost_fn: BCELoss
