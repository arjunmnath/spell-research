{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbxGTJLyRwpv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfTransformer,\n",
        "    TfidfVectorizer,\n",
        ")\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline, make_union\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "files = {\n",
        "   \"mal_full_offensive_train.csv\": \"1TX-2hn2dsFvmrU-t1PULOYeAF96SnI0h\",\n",
        "   \"mal_full_offensive_dev.csv\": '1p4VRR9pP-WvOGh36Pee4iQm__jdCtQAk'\n",
        "}\n",
        "for file_name, file_id in files.items():\n",
        "  url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "  gdown.download(url, file_name, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BG1sMghR9fY",
        "outputId": "66decc2d-6ad1-4b4b-9f17-218aacbae86f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1TX-2hn2dsFvmrU-t1PULOYeAF96SnI0h\n",
            "To: /content/mal_full_offensive_train.csv\n",
            "100%|██████████| 2.02M/2.02M [00:00<00:00, 77.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1p4VRR9pP-WvOGh36Pee4iQm__jdCtQAk\n",
            "To: /content/mal_full_offensive_dev.csv\n",
            "100%|██████████| 258k/258k [00:00<00:00, 27.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MuRILVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, batch_size=512):\n",
        "        self.model_name = \"setu4993/LaBSE\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModel.from_pretrained(self.model_name)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.Series):\n",
        "            X = X.tolist()\n",
        "\n",
        "        embeddings = []\n",
        "        for i in range(0, len(X), self.batch_size):\n",
        "            batch = X[i:i + self.batch_size]\n",
        "            batch_embeddings = self.get_embeddings(batch)\n",
        "            embeddings.append(batch_embeddings)\n",
        "        return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "    def get_embeddings(self, texts):\n",
        "        inputs = self.tokenizer(texts, max_length=256, padding='max_length',  truncation=True, return_tensors='pt')\n",
        "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        out = outputs.last_hidden_state\n",
        "        mean_pooling = torch.mean(out, 1)\n",
        "        max_pooling, _ = torch.max(out, 1)\n",
        "        embed = torch.cat((mean_pooling, max_pooling), 1)\n",
        "        return embed.cpu().numpy()\n",
        "\n",
        "\n",
        "class BoCVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self.vectorizer.fit(X)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.vectorizer.transform(X)\n",
        "\n",
        "class Preprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, label_encoding=False):\n",
        "        self.label_encoding = label_encoding\n",
        "        self.encoder = LabelEncoder() if label_encoding else None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.label_encoding:\n",
        "            self.encoder.fit(y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        # Ensure X is a list of strings before lowercasing\n",
        "        X_list = X.tolist() if isinstance(X, pd.Series) else X\n",
        "        X_transformed = [str(text).lower() for text in X_list]\n",
        "\n",
        "        if self.label_encoding and y is not None:\n",
        "            y_transformed = self.encoder.transform(y)\n",
        "            return X_transformed, y_transformed\n",
        "        return X_transformed\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        # Ensure X is a list of strings before lowercasing\n",
        "        return X_transformed, y # return original y if no label encoding\n",
        "\n",
        "feature_sets = {\n",
        "    # \"Word2Vec\": make_pipeline(Preprocessor(), Word2VecVectorizer()),\n",
        "    \"LaBSE Embedding\": make_pipeline(MuRILVectorizer()),\n",
        "    # \"BoW\": make_pipeline(CountVectorizer(token_pattern=r'[\\u0D00-\\u0D7F]+|[a-zA-Z]+')),\n",
        "    # \"TF-IDF\": make_pipeline(TfidfVectorizer(token_pattern=r'[\\u0D00-\\u0D7F]+|[a-zA-Z]+')),\n",
        "}\n",
        "classifiers = {\n",
        "    \"RFC\": make_pipeline(RandomForestClassifier(n_estimators=500)),\n",
        "    \"AdaBoost\": make_pipeline(StandardScaler(with_mean=False), AdaBoostClassifier()),\n",
        "    \"Bagging\": make_pipeline(StandardScaler(with_mean=False), BaggingClassifier()),\n",
        "    \"DecisionTree\": make_pipeline(StandardScaler(with_mean=False), DecisionTreeClassifier()),\n",
        "    \"ExtraTrees\": make_pipeline(StandardScaler(with_mean=False), ExtraTreesClassifier()),\n",
        "    \"KNeighbors\": make_pipeline(\n",
        "        StandardScaler(with_mean=False), KNeighborsClassifier()\n",
        "    ),\n",
        "    \"LinearSVC\": make_pipeline(\n",
        "        StandardScaler(with_mean=False), SVC(kernel=\"linear\",  probability=True)\n",
        "    ),\n",
        "    \"LogisticRegression\": make_pipeline(\n",
        "        StandardScaler(with_mean=False), LogisticRegression(max_iter=1000)\n",
        "    ),\n",
        "    \"MLP\": make_pipeline(\n",
        "        StandardScaler(with_mean=False), MLPClassifier(max_iter=1000)\n",
        "    ),\n",
        "    \"NearestCentroid\": make_pipeline(StandardScaler(with_mean=False), NearestCentroid()),\n",
        "    \"OneVsOne\": make_pipeline(StandardScaler(with_mean=False), OneVsOneClassifier(LinearSVC())),\n",
        "    \"OneVsRest\": make_pipeline(StandardScaler(with_mean=False), OneVsRestClassifier(LinearSVC())),\n",
        "    \"PassiveAggressive\": make_pipeline(\n",
        "        StandardScaler(with_mean=False), PassiveAggressiveClassifier(max_iter=1000)\n",
        "    ),\n",
        "    \"Perceptron\": make_pipeline(StandardScaler(with_mean=False), Perceptron(max_iter=1000)),\n",
        "    \"RidgeClassifier\": make_pipeline(StandardScaler(with_mean=False), RidgeClassifier()),\n",
        "    \"SGDClassifier\": make_pipeline(\n",
        "        StandardScaler(with_mean=False), SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "    ),\n",
        "    \"SVC-GC\": make_pipeline(\n",
        "        StandardScaler(with_mean=False), SVC(kernel=\"poly\",  probability=True)\n",
        "    ),\n",
        "    \"SVM_RBF\": make_pipeline(StandardScaler(with_mean=False), SVC(kernel=\"rbf\",  probability=True)),\n",
        "}"
      ],
      "metadata": {
        "id": "QOTMRZQ5SP6_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                             confusion_matrix, roc_auc_score, precision_recall_fscore_support)\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "import re\n",
        "from sklearn.metrics import f1_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "OzdcKhfcSajL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def train():\n",
        "    np.set_printoptions(precision=5)\n",
        "    pd.set_option(\"display.float_format\", \"{:.5f}\".format)\n",
        "    train_df = pd.read_csv(\"/content/mal_full_offensive_train.csv\") # Swapped to use train file for training\n",
        "    test_df = pd.read_csv(\"/content/mal_full_offensive_dev.csv\") # Swapped to use dev file for testing\n",
        "\n",
        "    X_train, X_test, y_train, y_test = (\n",
        "        train_df[\"Text\"],\n",
        "        test_df[\"Text\"],\n",
        "        train_df[\"Labels\"],\n",
        "        test_df[\"Labels\"],\n",
        "    )\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "    feature_cache = {}\n",
        "    results = []\n",
        "    try:\n",
        "      for f_name, f_pipe in feature_sets.items():\n",
        "        if f_name not in feature_cache:\n",
        "            X_train_transformed = f_pipe.fit_transform(X_train, y_train)\n",
        "            X_test_transformed = f_pipe.transform(X_test)\n",
        "            feature_cache[f_name] = X_train_transformed, X_test_transformed\n",
        "        X_feat_train, X_feat_test = feature_cache[f_name]\n",
        "        for c_name, clf in classifiers.items():\n",
        "            clf.fit(X_feat_train, y_train)\n",
        "            y_pred = clf.predict(X_feat_test)\n",
        "            y_true = y_test\n",
        "            acc = accuracy_score(y_true, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                y_true, y_pred, average='weighted', zero_division=0\n",
        "            )\n",
        "            f1_macro = f1_score(y_true, y_pred, average=\"macro\")  # Macro F1\n",
        "            f1_micro = f1_score(y_true, y_pred, average=\"micro\")  # Micro F1\n",
        "            ytest_bin = label_binarize(y_true, classes=[0, 1, 2, 3, 4])  # Adjust classes accordingly\n",
        "            ypred_bin = label_binarize(y_pred, classes=[0, 1, 2, 3, 4])  # Adjust classes accordingly\n",
        "\n",
        "            fpr = {}\n",
        "            tpr = {}\n",
        "            roc_auc = {}\n",
        "            for i in range(ytest_bin.shape[1]):\n",
        "                fpr[i], tpr[i], _ = roc_curve(ytest_bin[:, i], ypred_bin[:, i])\n",
        "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "            fpr[\"macro\"], tpr[\"macro\"], _ = roc_curve(ytest_bin.ravel(), ypred_bin.ravel())\n",
        "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "            print(\n",
        "                f\"{f_name} + {c_name} → Acc: {acc:.4f}, Prec: {precision:.4f}, \"\n",
        "                f\"Recall: {recall:.4f}, F1: {f1:.4f} \"\n",
        "                f\"F1_macro: {f1_macro:.4f}, F1_micro: {f1_micro:.4f}, \"\n",
        "                f\"ROC_AUC:{roc_auc['macro']:.4f}\"\n",
        "            )\n",
        "            results.append({\n",
        "                \"feature\": f_name,\n",
        "                \"classifier\": c_name,\n",
        "                \"accuracy\": acc,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall,\n",
        "                \"f1\": f1,\n",
        "                'f1_macro': f1_macro,\n",
        "                'f1_micro': f1_micro,\n",
        "                'roc_auc': roc_auc[\"macro\"],\n",
        "            })\n",
        "    except KeyboardInterrupt:\n",
        "      return results\n",
        "    return results\n",
        "if __name__ == \"__main__\":\n",
        "    results = train()\n",
        "    clear_output(wait=True)\n",
        "    df = pd.DataFrame(results)\n",
        "    df = df.sort_values(by=[\"f1\"], ascending=False)\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(df)\n",
        "    df.to_csv(\"results.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZZIzyknSdL9",
        "outputId": "99cc6dd4-9230-4266-b012-a9ab4786e97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LaBSE Embedding + RFC → Acc: 0.9555, Prec: 0.9572, Recall: 0.9555, F1: 0.9504 F1_macro: 0.7561, F1_micro: 0.9555, ROC_AUC:0.9722\n",
            "LaBSE Embedding + AdaBoost → Acc: 0.9070, Prec: 0.8726, Recall: 0.9070, F1: 0.8828 F1_macro: 0.2820, F1_micro: 0.9070, ROC_AUC:0.9418\n",
            "LaBSE Embedding + Bagging → Acc: 0.9480, Prec: 0.9475, Recall: 0.9480, F1: 0.9421 F1_macro: 0.6710, F1_micro: 0.9480, ROC_AUC:0.9675\n"
          ]
        }
      ]
    }
  ]
}