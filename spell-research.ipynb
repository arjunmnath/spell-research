{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gdown\nfiles = {\n   \"mal_full_offensive_train.csv\": \"1RZ8vMy7SrCl70X7CWDD5LEmwLcYOxSpn\",\n   \"mal_full_offensive_dev.csv\": '1wT5EM_k-D81sZFTf0JPRHHm-2Lr7nxiR'\n}\nfor file_name, file_id in files.items():\n  url = f'https://drive.google.com/uc?export=download&id={file_id}'\n  gdown.download(url, file_name, quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:13.440890Z","iopub.execute_input":"2025-07-31T10:56:13.441147Z","iopub.status.idle":"2025-07-31T10:56:17.961004Z","shell.execute_reply.started":"2025-07-31T10:56:13.441126Z","shell.execute_reply":"2025-07-31T10:56:17.960333Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1RZ8vMy7SrCl70X7CWDD5LEmwLcYOxSpn\nTo: /kaggle/working/mal_full_offensive_train.csv\n100%|██████████| 2.02M/2.02M [00:00<00:00, 131MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1wT5EM_k-D81sZFTf0JPRHHm-2Lr7nxiR\nTo: /kaggle/working/mal_full_offensive_dev.csv\n100%|██████████| 258k/258k [00:00<00:00, 94.3MB/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nimport pandas as pd\nfrom tqdm import tqdm \nimport random\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:17.962159Z","iopub.execute_input":"2025-07-31T10:56:17.962412Z","iopub.status.idle":"2025-07-31T10:56:34.511006Z","shell.execute_reply.started":"2025-07-31T10:56:17.962397Z","shell.execute_reply":"2025-07-31T10:56:34.510404Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed) \n    torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n# set_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.511750Z","iopub.execute_input":"2025-07-31T10:56:34.512214Z","iopub.status.idle":"2025-07-31T10:56:34.516515Z","shell.execute_reply.started":"2025-07-31T10:56:34.512185Z","shell.execute_reply":"2025-07-31T10:56:34.515745Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"## hyperparameters\ndevice = 'cuda' if torch.cuda.is_available() else \"cpu\" \nembed_vector_size = 256\nnum_classes = 5\nlr = 1e-4\nlr_step_size = 2\nlr_decay = 0.9\nearly_stopping_patience = 5\nnum_epochs = 50\nmodel_name = \"google/muril-base-cased\"\nnum_heads = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.518073Z","iopub.execute_input":"2025-07-31T10:56:34.518644Z","iopub.status.idle":"2025-07-31T10:56:34.621230Z","shell.execute_reply.started":"2025-07-31T10:56:34.518614Z","shell.execute_reply":"2025-07-31T10:56:34.620636Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df = pd.read_csv(\"mal_full_offensive_train.csv\")\nclass_weights = torch.tensor(1e4 / train_df.Labels.value_counts().values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.622066Z","iopub.execute_input":"2025-07-31T10:56:34.622304Z","iopub.status.idle":"2025-07-31T10:56:34.753209Z","shell.execute_reply.started":"2025-07-31T10:56:34.622281Z","shell.execute_reply":"2025-07-31T10:56:34.752636Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class OffensiveDataset(Dataset):\n    def __init__(self, path, device=\"cpu\"):\n        self._path = path\n        assert path.endswith('.csv'), \"expected a csv file\"\n        self.df = pd.read_csv(path)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.classes = list(self.df.Labels.unique())\n        self.classes.reverse()\n        self.nclasses = len(self.classes)\n        self.columns = self.df.columns\n    def __len__(self) -> int:\n        return self.df.shape[0]\n\n    def get_label(self, labels):\n        assert isinstance(labels, np.ndarray)\n        assert labels.ndim == 1\n        batch_size = labels.shape[0]\n        vec = torch.zeros(batch_size, self.nclasses)\n        indices = torch.tensor([self.classes.index(label) for label in labels])\n        vec[torch.arange(batch_size), indices] = 1\n        return vec.squeeze()\n        \n    def __getitem__(self, idx):\n        sentences = self.df.Text.iloc[idx] if isinstance(idx, int) else self.df.Text.iloc[idx].tolist()\n        labels = np.array([self.df.Labels.iloc[idx]]) if isinstance(idx, int) else self.df.Labels.iloc[idx].values\n        return self.tokenize(sentences, max_length=embed_vector_size), self.get_label(labels)\n        \n    def tokenize(self, sentences, max_length=256):\n        inputs = self.tokenizer(sentences, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n        inputs = {key: value.squeeze() for key, value in inputs.items()}\n        return tuple(inputs.values())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.753995Z","iopub.execute_input":"2025-07-31T10:56:34.754231Z","iopub.status.idle":"2025-07-31T10:56:34.761992Z","shell.execute_reply.started":"2025-07-31T10:56:34.754209Z","shell.execute_reply":"2025-07-31T10:56:34.761319Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self, device='cpu'):\n        super(Embedding, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.output_vector_size = self.model.config.hidden_size\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        output = self.model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask = attention_mask)\n        return output.last_hidden_state[:, 0, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.762694Z","iopub.execute_input":"2025-07-31T10:56:34.763046Z","iopub.status.idle":"2025-07-31T10:56:34.784226Z","shell.execute_reply.started":"2025-07-31T10:56:34.763023Z","shell.execute_reply":"2025-07-31T10:56:34.783499Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class FFNClassifier(nn.Module):\n    def __init__(self, num_classes, hidden_size=512, dropout_prob=0.2):\n        super(FFNClassifier, self).__init__()\n        self.embed = Embedding()\n        self.dropout = nn.Dropout(dropout_prob)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        self.fc1 = nn.Linear(self.embed.output_vector_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        self.layernorm = nn.LayerNorm(hidden_size)\n\n        for param in self.embed.parameters():\n            param.requires_grad = False\n\n    def forward(self,input_ids, token_type_ids, attention_mask):\n        x = self.embed(input_ids, token_type_ids, attention_mask)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = self.layernorm(x)\n        x = self.relu(x)\n        logits = self.fc2(x)\n        return self.softmax(logits)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.784978Z","iopub.execute_input":"2025-07-31T10:56:34.785224Z","iopub.status.idle":"2025-07-31T10:56:34.802061Z","shell.execute_reply.started":"2025-07-31T10:56:34.785200Z","shell.execute_reply":"2025-07-31T10:56:34.801538Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class BiLSTMClassifier(nn.Module):\n    def __init__(self, num_classes, hidden_size=512, dropout_prob=0.2):\n        super(BiLSTMClassifier, self).__init__()\n        self.embed = Embedding()\n        self.bilstm = nn.LSTM(self.embed.output_vector_size, hidden_size, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc1 = nn.Linear(hidden_size * 2, 512) \n        self.fc2 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n        for param in self.embed.parameters():\n            param.requires_grad = False\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        x = self.embed(input_ids, token_type_ids, attention_mask)\n        x = x.unsqueeze(1)\n        lstm_out, _ = self.bilstm(x)\n        lstm_out = lstm_out[:, -1, :]  \n        x = self.fc1(lstm_out)\n        x = self.dropout(x)\n        x = self.relu(x)\n        logits = self.fc2(x)\n        return self.softmax(logits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.802684Z","iopub.execute_input":"2025-07-31T10:56:34.802988Z","iopub.status.idle":"2025-07-31T10:56:34.824822Z","shell.execute_reply.started":"2025-07-31T10:56:34.802971Z","shell.execute_reply":"2025-07-31T10:56:34.824284Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class HANClassifier(nn.Module):\n    def __init__(self, num_classes, hidden_size=512, dropout_prob=0.2, num_heads=4):\n        super(HANClassifier, self).__init__()\n        self.embed = Embedding()\n\n        self.word_rnn = nn.LSTM(self.embed.output_vector_size, hidden_size, batch_first=True)\n        self.word_attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n        \n        self.sentence_rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n        self.sentence_attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n        \n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc1 = nn.Linear(hidden_size, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n        for param in self.embed.parameters():\n            param.requires_grad = False\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        x = self.embed(input_ids, token_type_ids, attention_mask)\n        x = x.unsqueeze(1)\n        word_out, _ = self.word_rnn(x)\n        word_out = word_out.permute(1, 0, 2)\n        word_attended, _ = self.word_attention(word_out, word_out, word_out)  \n        word_attended = word_attended.permute(1, 0, 2)  \n        word_attended = word_attended.mean(dim=1)  \n        \n        sentence_out, _ = self.sentence_rnn(word_attended.unsqueeze(1)) \n        sentence_out = sentence_out.permute(1, 0, 2) \n        sentence_attended, _ = self.sentence_attention(sentence_out, sentence_out, sentence_out)  \n        sentence_attended = sentence_attended.permute(1, 0, 2)  \n        sentence_attended = sentence_attended.mean(dim=1)  \n        \n        x = self.fc1(sentence_attended)\n        x = self.dropout(x)\n        x = self.relu(x)\n        logits = self.fc2(x)\n        return self.softmax(logits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.826450Z","iopub.execute_input":"2025-07-31T10:56:34.827014Z","iopub.status.idle":"2025-07-31T10:56:34.841411Z","shell.execute_reply.started":"2025-07-31T10:56:34.826997Z","shell.execute_reply":"2025-07-31T10:56:34.840760Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_csv = \"./mal_full_offensive_train.csv\"\ntest_csv = \"./mal_full_offensive_dev.csv\"\n\ntrain_dataset = OffensiveDataset(train_csv, device=device)\ntest_dataset = OffensiveDataset(test_csv, device=device)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:34.842146Z","iopub.execute_input":"2025-07-31T10:56:34.842434Z","iopub.status.idle":"2025-07-31T10:56:36.755135Z","shell.execute_reply.started":"2025-07-31T10:56:34.842418Z","shell.execute_reply":"2025-07-31T10:56:36.754521Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b332fef8c59466488cbc04ef9d5d47d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f999f4c043645faa7bf12c5b0710d32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce775e7cb1f8402daa2488a091f31798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"633b9d05f6364ba5ad48e48b3e6f6bd7"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"for _, labels in train_loader:\n    _, encoded = torch.max(labels, dim=1)\n    encoded = pd.Series(encoded.numpy())\n    print(encoded.value_counts())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:36.755840Z","iopub.execute_input":"2025-07-31T10:56:36.756054Z","iopub.status.idle":"2025-07-31T10:56:36.895493Z","shell.execute_reply.started":"2025-07-31T10:56:36.756038Z","shell.execute_reply":"2025-07-31T10:56:36.894840Z"}},"outputs":[{"name":"stdout","text":"4    60\n2     4\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"e = FFNClassifier(num_classes = num_classes)\ne(*train_dataset[:10][0]).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:56:36.896183Z","iopub.execute_input":"2025-07-31T10:56:36.896409Z","iopub.status.idle":"2025-07-31T10:57:20.655924Z","shell.execute_reply.started":"2025-07-31T10:56:36.896381Z","shell.execute_reply":"2025-07-31T10:57:20.655091Z"}},"outputs":[{"name":"stderr","text":"2025-07-31 10:56:52.792239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753959413.148575      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753959413.250152      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad2eb8be30845ef8e585c2216d5c4e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe6e88c63184f7896e3ba16bae4fca7"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"torch.Size([10, 5])"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"for (input_ids, token_type_ids, attention_mask), label in train_loader:\n    print(input_ids.shape, token_type_ids.shape, attention_mask.shape, label.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:57:20.656639Z","iopub.execute_input":"2025-07-31T10:57:20.658065Z","iopub.status.idle":"2025-07-31T10:57:20.718874Z","shell.execute_reply.started":"2025-07-31T10:57:20.658031Z","shell.execute_reply":"2025-07-31T10:57:20.718135Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 256]) torch.Size([64, 256]) torch.Size([64, 256]) torch.Size([64, 5])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# model = Classifier(num_classes=num_classes).to(device) \n# model = HANClassifier(num_classes=num_classes, num_heads=num_heads).to(device) \nmodel = BiLSTMClassifier(num_classes=num_classes).to(device)\nmodel = nn.DataParallel(model)\ncriterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\noptimizer = optim.AdamW(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=lr_step_size, gamma=lr_decay)  \nbest_val_loss = float('inf')\npatience_counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:57:20.719603Z","iopub.execute_input":"2025-07-31T10:57:20.719802Z","iopub.status.idle":"2025-07-31T10:57:26.219631Z","shell.execute_reply.started":"2025-07-31T10:57:20.719767Z","shell.execute_reply":"2025-07-31T10:57:26.218987Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for (input_ids, token_type_ids, attention_mask), label in tqdm(train_loader):\n        inputs_ids = input_ids.to(device)\n        token_type_ids = token_type_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        label = label.to(device)\n        \n        optimizer.zero_grad()\n\n        outputs = model(input_ids, token_type_ids, attention_mask)\n        loss = criterion(outputs, label)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        _, actual = torch.max(label, 1)\n        correct += (predicted == actual).sum().item()\n        total += label.size(0)\n\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct / total\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    y_true = []  \n    y_pred = []  \n    with torch.no_grad():\n        for (input_ids, token_type_ids, attention_mask), label in tqdm(test_loader):\n            inputs_ids = input_ids.to(device)\n            token_type_ids = token_type_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            label = label.to(device)\n            outputs = model(input_ids, token_type_ids, attention_mask)\n            loss = criterion(outputs, label)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            _, actual = torch.max(label, 1)\n            correct += (predicted == actual).sum().item()\n            y_true.extend(actual.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n            total += label.size(0)\n\n    val_loss /= len(test_loader)\n    val_accuracy = correct / total\n\n    print(f'Epoch {epoch+1}/{num_epochs} - '\n          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} - '\n          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n    # print(\"\\nClassification Report:\")\n    # print(classification_report(y_true, y_pred))\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stopping_patience:\n            print(\"Early stopping triggered\")\n            break\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:57:26.220514Z","iopub.execute_input":"2025-07-31T10:57:26.220728Z"}},"outputs":[{"name":"stderr","text":" 18%|█▊        | 44/251 [00:30<02:16,  1.52it/s]","output_type":"stream"}],"execution_count":null}]}