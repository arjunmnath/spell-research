{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Transformer_Combined.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"64fb7c3624f54fbabc704ab6969be147":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c612cbd1924b4413bfd78447d870dbab","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5a650871a286475693488236de641a70","IPY_MODEL_9a965bfc3f63449893d8db24f1f085a0"]}},"c612cbd1924b4413bfd78447d870dbab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a650871a286475693488236de641a70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6958c2bbc0c240989b7b181ffd58d971","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":512,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":512,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_12aa02b51488408586344ad8495b2356"}},"9a965bfc3f63449893d8db24f1f085a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77feb7fb6d2b491fbe475a61ea304999","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 512/512 [00:00&lt;00:00, 929B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0628a7c41f4b4375b1517ca4540eb7be"}},"6958c2bbc0c240989b7b181ffd58d971":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"12aa02b51488408586344ad8495b2356":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77feb7fb6d2b491fbe475a61ea304999":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0628a7c41f4b4375b1517ca4540eb7be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3254ab336208493f84a428c40dceceb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6dc31f44cb4542fb8a5ff69f631f0b7f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_930f266e093c478094a948801b39ede3","IPY_MODEL_0a0be45164e94f61b6d15f060bdec062"]}},"6dc31f44cb4542fb8a5ff69f631f0b7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"930f266e093c478094a948801b39ede3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6eba6089c09142cfabd132026f4b92f7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28d6c441219a4f8fa311346f2119ec54"}},"0a0be45164e94f61b6d15f060bdec062":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e580b3927c3b402e823245f036960e94","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.07M/5.07M [00:01&lt;00:00, 4.11MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aa57eabc87b64d3db3fe3ffcc9aea236"}},"6eba6089c09142cfabd132026f4b92f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"28d6c441219a4f8fa311346f2119ec54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e580b3927c3b402e823245f036960e94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"aa57eabc87b64d3db3fe3ffcc9aea236":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b827f9623cc043bf9921b4ea2372ea27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5b21dd33a9fd4bc982bc5d8c440f598a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3bd66fe23e7847d68113f29758eb7a3a","IPY_MODEL_9b58bdd6c9c84905b48bd7179d8bd350"]}},"5b21dd33a9fd4bc982bc5d8c440f598a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3bd66fe23e7847d68113f29758eb7a3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d0b48b40e2ae4e1fa29620a3c454000a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1115590446,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1115590446,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac020713bab242e68d4914bbdb0bee6b"}},"9b58bdd6c9c84905b48bd7179d8bd350":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ef6f3c35f05947c1b80795a674c49cdd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.12G/1.12G [00:27&lt;00:00, 39.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b23d311931014921a58a1dc7522c8e06"}},"d0b48b40e2ae4e1fa29620a3c454000a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ac020713bab242e68d4914bbdb0bee6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef6f3c35f05947c1b80795a674c49cdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b23d311931014921a58a1dc7522c8e06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/arjunmnath/indic-trans.git\n%cd indic-trans/\n!pip install -r requirements.txt\n!python setup.py install\n\n\nimport sys\nimport os\nfrom IPython.display import clear_output\n\ndst = [path for path in sys.path if 'site-packages' in path or 'dist-packages' in path][0]\nsrc = [path for path in os.listdir('build') if 'lib' in path][0]\nimport shutil\nshutil.move(f\"./build/{src}\", dst)\n%cd ..\nclear_output(wait=True)\nprint()\nfrom indictrans import Transliterator\ntrn = Transliterator(source='mal', target='eng', build_lookup=True)\ntrn.transform(\"നാട്ടിൽ എവിടാ?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T06:06:25.195698Z","iopub.execute_input":"2025-08-18T06:06:25.195995Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'indic-trans'...\nremote: Enumerating objects: 2191, done.\u001b[K\nremote: Counting objects: 100% (4/4), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nReceiving objects:  34% (745/2191), 158.93 MiB | 24.66 MiB/s\r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install sentencepiece==0.1.94\n!pip install demoji\n!pip install tweet-preprocessor\n# !pip install transformers[sentencepiece]","metadata":{"id":"0n47rpqSqWne","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610288059409,"user_tz":-330,"elapsed":198428,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"ded44783-4a1a-4479-f1e9-6a7e25162276","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gdown\nfolder_ids = [\"1GGVncH-e6J_dfi6IVixC7LpM7mBjJZLJ\", '1UccJUHZqvLD39kkoAB5BIaOtBNwYf4vZ']\nfor id in folder_ids:\n    gdown.download_folder(id=id)\nclear_output(wait=True); print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom transformers import AutoModel, AutoTokenizer\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport copy\nfrom transformers import BertModel, RobertaModel, BertTokenizer, RobertaTokenizer, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, random_split, DataLoader, IterableDataset, ConcatDataset\nimport sklearn\nfrom torch.optim import AdamW\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport demoji \nimport random\ndemoji.download_codes() \nimport preprocessor as p\n# from indictrans import Transliterator\np.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED)\nplt.rcParams['figure.figsize'] = [15, 8]\nplt.rcParams.update({'font.size': 8})\nRANDOM_SEED = 42\nmodel_path = 'ai4bharat/indic-bert'\nmodel_path = 'xlm-roberta-base'\nmodel_path = 'setu4993/LaBSE'\n# model_path = 'bert-base-multilingual-cased'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"9vOLxNuZlk4m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610288144191,"user_tz":-330,"elapsed":6965,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"18361cca-570f-4a31-d498-dcd6e9c6454d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_seed(seed_value, use_cuda):\n    np.random.seed(seed_value)  \n    torch.manual_seed(seed_value)  \n    random.seed(seed_value)\n    if use_cuda:\n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)  \n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nrandom_seed(RANDOM_SEED, True)","metadata":{"id":"72QHLh_2T-dA","executionInfo":{"status":"ok","timestamp":1610288144193,"user_tz":-330,"elapsed":6795,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset_OLID():\n    def __init__(self, train_data, batch_size = 32):\n        self.train_data = train_data\n        self.batch_size = batch_size\n\n        self.label_dict = {'Not_offensive': 0,\n                            'Offensive_Targeted_Insult_Group': 3,\n                            'Offensive_Targeted_Insult_Individual': 2,\n                            'Offensive_Targeted_Insult_Other': 4,\n                            'Offensive_Untargetede': 1}\n                                    \n        self.count_dic = {}\n        self.train_dataset = self.process_data(self.train_data)\n\n    def tokenize(self, sentences, padding = True, max_len = 256):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        input_ids, attention_masks = [], []\n        for sent in sentences:\n            encoded_dict = tokenizer.encode_plus(sent,\n                                                    add_special_tokens=True,\n                                                    max_length=max_len, \n                                                    padding='max_length', \n                                                    return_attention_mask = True,\n                                                    return_tensors = 'pt', \n                                                    truncation = True)\n            input_ids.append(encoded_dict['input_ids'])\n            attention_masks.append(encoded_dict['attention_mask'])\n        \n        input_ids = torch.cat(input_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n\n        return {'input_ids': (input_ids), 'attention_masks': (attention_masks)}\n    \n    def process_data(self, data):\n        sentences, labels = [], []\n        print(len(data))\n        for id,line in enumerate(data):\n            if id==0: continue\n            sentence = line.strip().split('\\t')\n            label = sentence[2:]\n\n            if label[0] == 'NOT': labels.append(0)\n            elif label[1] == 'UNT': labels.append(1)\n            elif label[2] == 'IND': labels.append(2)\n            elif label[2] == 'GRP': labels.append(3)\n            else: labels.append(4)\n\n            sentence = sentence[1].replace('#','').lower()\n            emoji_dict = demoji.findall(sentence)\n            if len(emoji_dict): \n                for emoji, text in emoji_dict.items():\n                    sentence = sentence.replace(emoji, ' '+text+' ')\n                    sentence = ' '.join(sentence.split())\n            sentences.append(sentence)\n            self.count_dic[labels[-1]] = self.count_dic.get(labels[-1], 0) + 1\n        inputs = self.tokenize(sentences)\n        return TensorDataset(inputs['input_ids'], inputs['attention_masks'], torch.Tensor(labels))\n    \n    def get_dataloader(self, inputs, labels, train = True):\n        data = TensorDataset(inputs['input_ids'], inputs['attention_masks'], labels)\n        if train:\n            sampler = RandomSampler(data)\n        else:\n            sampler = SequentialSampler(data)\n        return DataLoader(data, sampler=sampler, batch_size=self.batch_size)","metadata":{"id":"aC01VIjXBDaX","executionInfo":{"status":"ok","timestamp":1610288144194,"user_tz":-330,"elapsed":5020,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('olid/olid-training-v1.0.tsv', 'r') as f:\n    train_data = f.readlines()\nolid_data = Dataset_OLID(train_data)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":135,"referenced_widgets":["64fb7c3624f54fbabc704ab6969be147","c612cbd1924b4413bfd78447d870dbab","5a650871a286475693488236de641a70","9a965bfc3f63449893d8db24f1f085a0","6958c2bbc0c240989b7b181ffd58d971","12aa02b51488408586344ad8495b2356","77feb7fb6d2b491fbe475a61ea304999","0628a7c41f4b4375b1517ca4540eb7be","3254ab336208493f84a428c40dceceb1","6dc31f44cb4542fb8a5ff69f631f0b7f","930f266e093c478094a948801b39ede3","0a0be45164e94f61b6d15f060bdec062","6eba6089c09142cfabd132026f4b92f7","28d6c441219a4f8fa311346f2119ec54","e580b3927c3b402e823245f036960e94","aa57eabc87b64d3db3fe3ffcc9aea236"]},"id":"peoybAjPBGhU","executionInfo":{"status":"ok","timestamp":1610288159144,"user_tz":-330,"elapsed":19274,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"5f509ac8-c511-43ef-82a5-672d576b8561","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr1 = Transliterator(source='tam', target='eng', build_lookup=True)\ntr2 = Transliterator(source='mal', target='eng', build_lookup=True)\ntr3 = Transliterator(source='kan', target='eng', build_lookup=True)","metadata":{"id":"LpfSw70pkm4z","executionInfo":{"status":"ok","timestamp":1610288159553,"user_tz":-330,"elapsed":19194,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset():\n    def __init__(self, train_data, val_data, batch_size = 32):\n        self.train_data = train_data\n        self.val_data = val_data\n        self.batch_size = batch_size\n\n        self.label_dict = {'Not_offensive': 0,\n                            'Offensive_Targeted_Insult_Group': 3,\n                            'Offensive_Targeted_Insult_Individual': 2,\n                            'Offensive_Targeted_Insult_Other': 4,\n                            'Offensive_Untargetede': 1}\n        self.count_dic = {}\n        self.train_dataset = self.process_data(self.train_data)\n        self.val_dataset = self.process_data(self.val_data)\n\n        \n        # self.train_dataloader = self.get_dataloader(self.train_inputs, self.train_labels)\n        # self.val_dataloader = self.get_dataloader(self.val_inputs, self. val_labels, train = False)\n\n    def tokenize(self, sentences, padding = True, max_len = 256):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        input_ids, attention_masks = [], []\n        for sent in sentences:\n            encoded_dict = tokenizer.encode_plus(sent,\n                                                    add_special_tokens=True,\n                                                    max_length=max_len, \n                                                    padding='max_length', \n                                                    return_attention_mask = True,\n                                                    return_tensors = 'pt', \n                                                    truncation = True)\n            input_ids.append(encoded_dict['input_ids'])\n            attention_masks.append(encoded_dict['attention_mask'])\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n        return {'input_ids': input_ids, 'attention_masks': attention_masks}\n    \n    def process_data(self, data):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        sentences, labels = [], []\n        for line in data:\n            sentence = line.strip().split('\\t')\n            label = sentence.pop()\n            if label not in self.label_dict: continue\n            # print('label found')\n            sentence = ((' '+tokenizer.sep_token+' ').join(sentence)).replace('#','').lower()\n            sentence = tr3.transform(tr2.transform(tr1.transform(sentence)))\n            # sentence = p.clean(' '.join(sentence)).replace('#','')\n            emoji_dict = demoji.findall(sentence)\n            if len(emoji_dict): \n                for emoji, text in emoji_dict.items():\n                    sentence = sentence.replace(emoji, ' '+text+' ')\n                    sentence = ' '.join(sentence.split())\n            sentences.append(sentence)\n            # if label =='Not_offensive': labels.append(0)\n            # else:\n            labels.append(self.label_dict[label])\n            self.count_dic[labels[-1]] = self.count_dic.get(labels[-1], 0) + 1\n        inputs = self.tokenize(sentences)\n\n        return TensorDataset(inputs['input_ids'], inputs['attention_masks'], torch.Tensor(labels))\n    \n    def get_dataloader(self, inputs, labels, train = True):\n        data = TensorDataset(inputs['input_ids'], inputs['attention_masks'], labels)\n        if train:\n            sampler = RandomSampler(data)\n        else:\n            sampler = SequentialSampler(data)\n        return DataLoader(data, sampler=sampler, batch_size=self.batch_size)","metadata":{"id":"VIOPL_OTrRy4","executionInfo":{"status":"ok","timestamp":1610288159553,"user_tz":-330,"elapsed":18744,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# with open('/kaggle/working/FIRE-2025/Kannada/kannada_offensive_train.csv', 'r') as f:\n#     train_data = f.readlines()\n# with open('/kaggle/working/FIRE-2025/Kannada/kannada_offensive_dev.csv', 'r') as f:\n#     val_data = f.readlines()\n# kan_data = Dataset(train_data, val_data)\n\nwith open('/kaggle/working/FIRE-2025/Malayalam/mal_full_offensive_train.csv', 'r') as f:\n    train_data = f.readlines()\nwith open('/kaggle/working/FIRE-2025/Malayalam/mal_full_offensive_dev.csv', 'r') as f:\n    val_data = f.readlines()\nmal_data = Dataset(train_data, val_data)\n\n# with open('/kaggle/working/FIRE-2025/Tamil/tamil_offensive_full_train.csv', 'r') as f:\n#     train_data = f.readlines()\n# with open('/kaggle/working/FIRE-2025/Tamil/tamil_offensive_full_dev.csv', 'r') as f:\n#     val_data = f.readlines()\n# tam_data = Dataset(train_data, val_data)","metadata":{"id":"0HBnekZFnnhe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610288289303,"user_tz":-330,"elapsed":148054,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"0f9eb471-8db3-4d0e-f96c-f18fd6e376a1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save and Load Functions\ndef save_metrics(save_path, epochs, model, optimizer, F1):\n\n    state_dict = {'model_state_dict': model.state_dict(),\n                  'optimizer_state_dict': optimizer.state_dict(),\n                  'epochs': epochs+1,\n                  'F1': F1}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\n\ndef load_metrics(load_path, model, optimizer):\n    try: \n        state_dict = torch.load(load_path, map_location=device)\n        model.load_state_dict(state_dict['model_state_dict'])\n        optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n    except: \n        state_dict = {}\n\n    print(f'Model loaded from <== {load_path}')\n    return state_dict.get('epochs', 0), state_dict.get('F1', 0)","metadata":{"id":"igSNYLB6D1G8","executionInfo":{"status":"ok","timestamp":1610288425483,"user_tz":-330,"elapsed":1011,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Embedding(torch.nn.Module):\n    def __init__(self):\n        super(Embedding, self).__init__()\n        self.embeddings = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(0.3)\n        self.output_vector_size = self.embeddings.config.hidden_size * 3\n\n    def forward(self, input_ids, mask):\n        outputs = self.embeddings(input_ids, mask)\n        out = outputs.last_hidden_state # -> (batch_size, num_words, 768)\n        mean_pooling = torch.mean(out, 1)\n        max_pooling, _ = torch.max(out, 1)\n        embed = torch.cat((out[:, 0, :], mean_pooling, max_pooling), 1) # -> (batch_size, 768 * 3)\n        y_pred = self.dropout(embed)\n        return y_pred\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, lstm_out, mask=None):\n        attn_weights = self.attn(lstm_out).squeeze(-1)\n\n        if mask is not None:\n            attn_weights = attn_weights.masked_fill(mask == 0, -1e9)\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        weighted_sum = torch.bmm(attn_weights.unsqueeze(1), lstm_out)\n        return weighted_sum.squeeze(1)\n\nclass BiLSTMAttentionClassifier(nn.Module):\n    def __init__(self, embed_dim, hidden_dim=128, num_classes=5, dropout=0.3):\n        super().__init__()\n        self.bilstm = nn.LSTM(embed_dim, hidden_dim, \n                              batch_first=True, bidirectional=True)\n        self.attention = Attention(hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # x: (batch, seq_len, embed_dim)\n        lstm_out, _ = self.bilstm(x)  # (batch, seq_len, 2*hidden_dim)\n        sentence_vec = self.attention(lstm_out, mask)  # (batch, 2*hidden_dim)\n        sentence_vec = self.dropout(sentence_vec)\n        return sentence_vec\n\n\nclass HANFE(nn.Module):\n    def __init__(self, input_vector_size, hidden_size=128, dropout_prob=0.3, num_heads=4):\n        super(HANFE, self).__init__()\n        self.word_rnn = nn.LSTM(input_vector_size, input_vector_size, batch_first=True)\n        self.word_attention = nn.MultiheadAttention(embed_dim=input_vector_size, num_heads=num_heads, batch_first=True)\n\n        self.sentence_rnn = nn.LSTM(input_vector_size, input_vector_size, batch_first=True)\n        self.sentence_attention = nn.MultiheadAttention(embed_dim=input_vector_size, num_heads=num_heads, batch_first=True)\n        self.project = nn.Linear(input_vector_size * 2, input_vector_size, bias=True)\n        \n    def forward(self, x):\n        x = x.unsqueeze(1)\n        word_out, _ = self.word_rnn(x)\n        word_out = word_out.permute(1, 0, 2)\n        word_attended, _ = self.word_attention(word_out, word_out, word_out)  \n        word_attended = word_attended.permute(1, 0, 2).squeeze(1)\n        \n        sentence_out, _ = self.sentence_rnn(x) \n        sentence_out = sentence_out.permute(1, 0, 2) \n        sentence_attended, _ = self.sentence_attention(sentence_out, sentence_out, sentence_out)  \n        sentence_attended = sentence_attended.permute(1, 0, 2).squeeze(1)\n        attended = torch.cat([word_attended, sentence_attended], dim=1)\n        out = self.project(attended)\n        return out\n\nclass Classifier(nn.Module):\n    def __init__(self, input_size=768 * 2, hidden_size=128, num_classes=5, dropout_prob=0.3):\n        super(Classifier, self).__init__()\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc1 = nn.Linear(input_size, hidden_size, bias=True)\n        self.fc2 = nn.Linear(hidden_size, num_classes, bias=True) # add the fc1 layer too\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = self.relu(x)\n        logits = self.fc2(x)\n        return logits\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embed = Embedding()\n        # self.fe = BiLSTMAttentionClassifier(self.embed.output_vector_size, 128)\n        self.classifier = Classifier(self.embed.output_vector_size)\n\n    def forward(self, input_ids, mask):\n        x = self.embed(input_ids, mask)\n        # x = self.fe(x)\n        logits = self.classifier(x)\n        return logits\n","metadata":{"id":"zJTMSkNXrcjI","executionInfo":{"status":"ok","timestamp":1610288426081,"user_tz":-330,"elapsed":1461,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n \ndef get_predicted(preds):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    return pred_flat\n \ndef evaluate(test_dataloader, model):\n    model.eval()\n    y_preds, y_test = np.array([]), np.array([])\n    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n        for batch in test_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device).long()\n            with torch.no_grad():        \n                ypred = model(b_input_ids, b_input_mask)\n            ypred = ypred.cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n            y_preds = np.hstack((y_preds, get_predicted(ypred)))\n            y_test = np.hstack((y_test, label_ids))\n\n    weighted_f1 = f1_score(y_test, y_preds, average='weighted')\n    return weighted_f1, y_preds, y_test\n \ndef train(training_dataloader, validation_dataloader, model, filepath, weights = None, learning_rate = 2e-5, epochs = 4, print_every = 10):\n    total_steps = len(training_dataloader) * epochs\n    torch.cuda.empty_cache()\n    scaler = torch.amp.GradScaler('cuda')\n    no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps = 1e-8)\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                                num_warmup_steps = 0, # Default value in run_glue.py\n                                                num_training_steps = total_steps)\n    \n    current_epoch, best_weighted_f1 = load_metrics(filepath, model, optimizer)\n    if weights == None:\n        criterion = nn.CrossEntropyLoss()\n    else:\n        criterion = nn.CrossEntropyLoss(weight=weights)\n    for epoch_i in range(current_epoch, epochs):\n        model.train()\n        for batch in tqdm(training_dataloader):\n            b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device).long()\n            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n                outputs = model(b_input_ids, b_input_mask)\n                loss = criterion(outputs, b_labels)\n\n            optimizer.zero_grad()\n            # loss.backward()\n            scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            # optimizer.step()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n \n        print('### Validation Set Stats')\n        weighted_f1, ypred, ytest = evaluate(validation_dataloader, model)\n        print(f\"  Weighted F1 {epoch_i}: {weighted_f1:.4f}\")\n        if weighted_f1 > best_weighted_f1:\n            best_weighted_f1 = weighted_f1\n            save_metrics(filepath, epoch_i, model.module.embed.embeddings, optimizer, weighted_f1)","metadata":{"id":"NFqtqQcPvli0","executionInfo":{"status":"ok","timestamp":1610288427925,"user_tz":-330,"elapsed":1673,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_dataset = ConcatDataset([olid_data.train_dataset, kan_data.train_dataset, mal_data.train_dataset, tam_data.train_dataset])\ntrain_dataset = ConcatDataset([mal_data.train_dataset, olid_data.train_dataset])\n# train_dataset = ConcatDataset([tam_data.train_dataset, olid_data.train_dataset])\nval_dataset = mal_data.val_dataset","metadata":{"id":"8I0rCdWUsV2t","executionInfo":{"status":"ok","timestamp":1610288429104,"user_tz":-330,"elapsed":706,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count_dic = {}\nfor data in train_dataset:\n    label = int(data[2])\n    count_dic[label] = count_dic.get(label, 0)+1\nweights = torch.Tensor([1+np.log(len(train_dataset)/count_dic[i]) for i in range(5)]).to(device)","metadata":{"id":"zCexT0UutcJv","executionInfo":{"status":"ok","timestamp":1610288444638,"user_tz":-330,"elapsed":16125,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYMEeB0grZtX","executionInfo":{"status":"ok","timestamp":1610288444640,"user_tz":-330,"elapsed":14137,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"fa266a0b-3345-45e9-a5f2-4185a2e5e59a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44rgiKy5oCks","executionInfo":{"status":"ok","timestamp":1610288444640,"user_tz":-330,"elapsed":13081,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"69a80666-3943-4739-a087-990dcaa7555b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(mal_data.train_dataset), len(olid_data.train_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\nval_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=128)","metadata":{"id":"cjCQc8metC-U","executionInfo":{"status":"ok","timestamp":1610288446664,"user_tz":-330,"elapsed":1263,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Model()\n# model = nn.DataParallel(model)\n# model = torch.compile(model)\n# optimizer = AdamW(model.parameters(), lr=3e-5, eps = 1e-8)\n# load_metrics('olid_kannada_mbert.pt', model, optimizer)","metadata":{"id":"YMgZPosD1sTc","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["b827f9623cc043bf9921b4ea2372ea27","5b21dd33a9fd4bc982bc5d8c440f598a","3bd66fe23e7847d68113f29758eb7a3a","9b58bdd6c9c84905b48bd7179d8bd350","d0b48b40e2ae4e1fa29620a3c454000a","ac020713bab242e68d4914bbdb0bee6b","ef6f3c35f05947c1b80795a674c49cdd","b23d311931014921a58a1dc7522c8e06"]},"executionInfo":{"status":"ok","timestamp":1610288480364,"user_tz":-330,"elapsed":34049,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"eb6cd633-7cb5-4866-d6bc-f1eefc94068c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.onnx\n\ninput_ids = torch.randn(1, 256)  # Batch size of 1, 10 input features\ndummy_input = (mal_data.train_dataset[:2][0], mal_data.train_dataset[:2][1])\ntorch.onnx.export(\n    model,                # The model to export\n    dummy_input,          # The dummy input tensor\n    \"model.onnx\",         # Path to save the ONNX file\n    export_params=True,   # Store the trained parameters (weights)\n    opset_version=11,     # ONNX opset version (choose the one that works best)\n    do_constant_folding=True,  # Whether to fold constants for optimization\n    input_names=['input'],    # Name of the input layer\n    output_names=['output']   # Name of the output layer\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(train_dataloader, val_dataloader, model, 'olid_xlmr_base_embed_new.pt', weights=weights, epochs=4)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0ufvYfIYwuS","executionInfo":{"status":"ok","timestamp":1610302974269,"user_tz":-330,"elapsed":8609958,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"18426363469360203926"}},"outputId":"fa50db00-49c0-412e-c626-c9905d32d631","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_, ypred, ytest = evaluate(val_dataloader , model)\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_auc_score, classification_report\narray = confusion_matrix(ytest, ypred)","metadata":{"id":"quTk_hanRRav","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ytest.shape, ypred.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"precision, recall, f1, _ = precision_recall_fscore_support(\n                ytest, ypred, average='weighted', zero_division=0\n            )\naccuracy = accuracy_score(ytest, ypred)\nf1_macro = f1_score(ytest, ypred, average=\"macro\")  # Macro F1\nf1_micro = f1_score(ytest, ypred, average=\"micro\")  # Micro F1\nytest_bin = label_binarize(ytest, classes=[0.0, 1.0, 2.0, 3.0])  # Adjust classes accordingly\nypred_bin = label_binarize(ypred, classes=[0.0, 1.0, 2.0, 3.0])  # Adjust classes accordingly\n\nfpr = {}\ntpr = {}\nroc_auc = {}\nfor i in range(ytest_bin.shape[1]):\n    fpr[i], tpr[i], _ = roc_curve(ytest_bin[:, i], ypred_bin[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"macro\"], tpr[\"macro\"], _ = roc_curve(ytest_bin.ravel(), ypred_bin.ravel())\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ytest_bin.ravel(), ypred_bin.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"precision: {precision: .4f}\")\nprint(f\"recall: {recall: .4f}\")\nprint(f\"Weighted F1: {f1: .4f}\")\nprint(f\"Macro F1: {f1_macro : .4f}\")\nprint(f\"Micro F1: {f1_micro : .4f}\")\nprint(f\"Macro AUC: {roc_auc['macro']: .4f}\")\nprint(f\"Micro AUC: {roc_auc['micro']: .4f}\")\nprint(f\"Macro-AUC: {roc_auc['macro']: .4f}\")\nprint(classification_report(ytest, ypred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(ytest, ypred)\nroc_auc = auc(fpr, tpr)\nprint(f\"AUC: {roc_auc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_cm = pd.DataFrame(array, range(5), range(5))\n# plt.figure(figsize=(10,7))\nsn.set(font_scale=1.4) # for label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"YZ588U8OTEsh","executionInfo":{"status":"ok","timestamp":1607866971747,"user_tz":-330,"elapsed":1292,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"66a3b8b8-694b-40da-9c8f-73469499dad2","trusted":true},"outputs":[],"execution_count":null}]}